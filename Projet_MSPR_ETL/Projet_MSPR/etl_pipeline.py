from prefect import flow, task
import pandas as pd
import os
import psycopg2
import time

# ðŸ“Œ Dossier contenant les fichiers CSV
DATA_FOLDER = "data"

# ðŸ“Œ Liste des fichiers CSV Ã  charger
DATASETS = {
    "covid": "WHO_COVID_DATA.csv",  # Nouveau fichier de l'OMS
    "mpox": "owid-monkeypox-data.csv"
}

# ðŸ“Œ Colonnes standardisÃ©es pour uniformiser les donnÃ©es
STANDARD_COLUMNS = ["country", "date", "cases", "deaths"]

# ðŸ“Œ VÃ©rification PostgreSQL
def check_postgres():
    print("ðŸ” VÃ©rification de la connexion Ã  PostgreSQL...")
    for _ in range(5):  
        try:
            conn = psycopg2.connect(
                dbname="pandemics",
                user="postgres",
                password="admin",
                host="localhost",
                port=5432
            )
            conn.close()
            print("âœ… Connexion PostgreSQL rÃ©ussie !")
            return True
        except psycopg2.OperationalError:
            print("âš ï¸ PostgreSQL inaccessible, nouvelle tentative dans 5s...")
            time.sleep(5)
    print("âŒ PostgreSQL inaccessible aprÃ¨s plusieurs tentatives.")
    return False

# ðŸ“Œ Ã‰tape 1 : Nettoyer et Uniformiser les DonnÃ©es
@task
def clean_data():
    os.makedirs(DATA_FOLDER, exist_ok=True)
    cleaned_files = {}

    for dataset, filename in DATASETS.items():
        file_path = os.path.join(DATA_FOLDER, filename)

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"âŒ Le fichier {filename} est introuvable.")

        print(f"ðŸ“‚ Chargement et nettoyage du fichier {filename}...")

        try:
            df = pd.read_csv(file_path, encoding="utf-8", delimiter=",", on_bad_lines="skip")  # âœ… Correction ici
        except Exception as e:
            print(f"âš ï¸ Erreur de lecture du fichier {filename}: {e}")
            continue

        # ðŸŒŸ Standardisation des colonnes (conversion dynamique)
        if dataset == "covid":
            df = df.rename(columns={
                "Country": "country",
                "Date_reported": "date",
                "New_cases": "cases",
                "New_deaths": "deaths"
            })
        elif dataset == "mpox":
            df = df.rename(columns={
                "location": "country",
                "date": "date",
                "new_cases": "cases",
                "total_deaths": "deaths"
            })
        
        # ðŸŒŸ VÃ©rification et correction des types de donnÃ©es
        df["date"] = pd.to_datetime(df["date"], errors='coerce')  # Convertir en datetime
        df["cases"] = pd.to_numeric(df["cases"], errors='coerce').fillna(0).astype(int)
        df["deaths"] = pd.to_numeric(df["deaths"], errors='coerce').fillna(0).astype(int)

        # ðŸŒŸ Suppression des valeurs invalides
        df = df.dropna(subset=["date"])  # Supprimer les dates invalides
        df = df[df["cases"] >= 0]  # Supprimer les valeurs nÃ©gatives
        df = df[df["deaths"] >= 0]

        # ðŸŒŸ Sauvegarde des fichiers nettoyÃ©s
        cleaned_file = os.path.join(DATA_FOLDER, f"{dataset}_cleaned.csv")
        df[STANDARD_COLUMNS].to_csv(cleaned_file, index=False)
        cleaned_files[dataset] = cleaned_file

        print(f"âœ… DonnÃ©es nettoyÃ©es et sauvegardÃ©es dans {cleaned_file}")

    return cleaned_files


# ðŸ“Œ Ã‰tape 2 : Charger les donnÃ©es dans PostgreSQL
@task
def load_to_postgres(cleaned_files):
    if not check_postgres():
        raise ConnectionError("Ã‰chec de connexion Ã  PostgreSQL.")

    conn = psycopg2.connect(
        dbname="pandemics",
        user="postgres",
        password="admin",
        host="localhost",
        port=5432
    )
    cursor = conn.cursor()
    
    # ðŸ“Œ CrÃ©ation des tables gÃ©nÃ©riques
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS pandemic_data (
            id SERIAL PRIMARY KEY, 
            country VARCHAR(100), 
            date DATE, 
            cases INT, 
            deaths INT, 
            disease VARCHAR(50)
        );
    """)

    # ðŸ“Œ Insertion dynamique des donnÃ©es nettoyÃ©es
    for dataset, file_path in cleaned_files.items():
        df = pd.read_csv(file_path)

        for _, row in df.iterrows():
            print(f"Insertion : {row['country']} | {row['date']} | {row['cases']} | {row['deaths']} | {dataset}")
            cursor.execute("""
                INSERT INTO pandemic_data (country, date, cases, deaths, disease) 
                VALUES (%s, %s, %s, %s, %s)
            """, (row["country"], row["date"], row["cases"], row["deaths"], dataset))

    conn.commit()
    cursor.close()
    conn.close()

    print("âœ… DonnÃ©es chargÃ©es dans PostgreSQL")

# ðŸ“Œ Ã‰tape 3 : DÃ©finition du flow Prefect
@flow
def etl_pipeline():
    cleaned_files = clean_data()
    load_to_postgres(cleaned_files)

# ðŸ“Œ ExÃ©cuter le pipeline
if __name__ == "__main__":
    etl_pipeline()
